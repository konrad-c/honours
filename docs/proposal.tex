\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{pifont}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{amssymb}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
	\hspace{-2.5pt}}

\title{Synthesizing emotive art}
\author{Konrad Cybulski}
\date{March 2019}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}
	
\maketitle


\section{Introduction}

In the domain of creative artificial intelligence and evolutionary art, the desire exists for processes that produce imagery that is not only visually appealing, but images that exhibit abstract and emotive characteristics.
There exists extensive research on the production of realistic, and target label accurate images.
The use of quality-diverse (QD) algorithms in combination with deep neural network (DNN) image classifiers were used by \citet{nguyen2015innovation} to generate images with high classification accuracy from pre-trained DNN.
\citet{bao2017cvae} exemplifies recent use of variational generative adversarial network (GAN) architectures for the generation of realistic images from fine-grained target labels.
GANs have shown their use in text-to-image synthesis \citep{reed2016generative, zhang2017stackgan}, producing realistic images reflecting the detailed description from which they are generated.
With regard to creative image generation, \citet{tan2017artgan} explored techniques for synthesising artwork with more abstract characteristics.
Through the use of a target artist or genre, the generative network produced images that were highly abstract and stylistically accurate.

Little exploration however has been done on incorporating emotion into the process of art and image generation.
\citet{ali2017emotional} explored the idea of \textit{emotion transfer}, using techniques such as image emotion assignment, and colour/style transfer with the aim of altering image composition to reach a target emotion.
Examples given use a target profile, with varying levels of emotions such as joy, anger, and fear, to alter the image's colour composition.
The classification of an image's affective emotion, the emotion with which a viewer classifies the image, has been explored in various works \citep{machajdik2010affective, chen2015learning, kim2018building}.
\citet{kim2018building} produced a classifier for recognising the emotion attributed to an image.
This was done through the application of a DNN to decompose an image to a two-dimensional feature vector (valence and arousal) representing the image's emotion mapped to a continuous plane (see Figure \ref{fig:valence-arousal}).

\section{Motivation}



\section{Aims}
The aim of this research is to better understand the visual patterns associated with various emotions conveyed in images.
This will primarily leverage image emotion recognition architectures explored by \citet{kim2018building}, in combination with the dataset produced by \citet{zhao2016predicting} containing over 1.4 million images with assigned valence, arousal, and dominance levels derived from their descriptions.
Producing an architecture with which valence-arousal (VA) values can be assigned to images forms the basis for this research.
Such a platform allows further exploration into the way in which it assigns VA values to more complex, multi-faceted and multi-layered images, and the efficacy with which this is done.
Furthermore this process, and the patterns learned by it can be better understood through the use of generative processes which maximise given target features in a quality-diverse way \citep{nguyen2015innovation, nguyen2015deep}, or through a generative adversarial approach \citep{tan2017artgan}.
In the context of this research, such features include valence, arousal, dominance, happiness, sadness, etc.
This will allow a great understanding of the visual patterns that such an architecture learns, and any psychological or artistic parallels that can be drawn.
The combination of such a generative system with a classifier of emotion can be further extended to domains such as generative art and text-to-image synthesis, with the focus of emotion-driven image generation.

\section{Background}

\subsection{Unsupervised image synthesis}
\begin{todolist}
	\item Image generation methods explored: evolutionary algorithms, neural networks, line drawing, etc.
	\item Measuring \textit{goodness} of a generated image: realism, abstractness, aesthetic appeal, etc.
\end{todolist}

\begin{figure}[h!]
	\includegraphics[width=\textwidth]{images/sims-interactive-image-generation.png}
	\caption{Images generated through the process of interactive evolution introduced by \citet{sims}}
	\label{fig:sims}
\end{figure}

The area of image and art generation has been explored through various avenues.
Some of the first \textit{human-in-the-loop} systems such as \textit{NEvAr} \citep{nevar} produced greatly impressive images  through evolutionary techniques.
Evolutionary art leveraged methods introduced and exemplified by \citet{sims} such as those shown in Figure \ref{fig:sims}.
\citet{sims} proposed using \textit{Lisp} expressions for genotype definitions, which accepted a coordinate (x, y) which could be evaluated into a grayscale or RGB value, thus producing images.
This genotype expression has been used in numerous further research into the process of both supervised and unsupervised image synthesis through evolutionary techniques \citep{nevar, sims, den2011evolving, distributed-evolutionary-art, aesthetic-measures}.

Despite the slow nature of the interactive process, \citet{sims} and \citet{nevar} were able to produce images with visually striking characteristics.
\citet{aesthetic-measures} investigated measures of aesthetics for fitness evaluation in artificially evolving images.
This research primarily used observations by \citet{ralph-bell-curve}, that the distribution of colour gradients in fine art tend towards normal.
While the images produced through this method did not meet the level of intricacy and detail as the results of \citet{sims} or \citet{nevar}, it represented a self-contained system able to generate appealing imagery without human interaction.

Introduction of the generative adversarial network architecture (GAN) by \citet{GAN} allowed the process of image generation to be completely unsupervised.
Common GAN application has involved the generation of realistic images, such as has been done by \citet{bao2017cvae}, where images have been synthesized to fine-detailed target labels such as bird species' and actors.
\citet{zhang2017stackgan} and \citet{reed2016generative} have recently explored text to image synthesis, in which detailed descriptions of birds and flowers have been converted into photo-realistic images using the GAN model.
\citet{tan2017artgan} has explored the generation of art according to target genre and artist.

\subsection{Generative adversarial networks}
\begin{todolist}
	\item What is the generative adversarial network, and differences in common architectures
	\item Why are GANs advantageous over the use of target feature analysis (aesthetics, etc.)
	\item Text-to-image
	\item Style transfer \& image-to-image
\end{todolist}

Deep neural networks (DNN) have grown tremendously in popularity within the domain of image generation, and classification.

	
\subsection{Image emotion recognition}
\begin{todolist}
	\item Sentiment classification: text \& images.
	\item Emotion classification in images: facial expression, general imagery.
	\item Methods of classifying emotion in images: single target emotion, discrete categorical likelihood, decomposition into continuous vector (valence-arousal).
\end{todolist}
	
\begin{figure}[h!]
	\includegraphics[width=0.75\textwidth]{images/valence-arousal-grid.png}
	\caption{Distribution of emotions associated with levels of valence and arousal determined by DNN classifier produced by \citet{kim2018building}}
	\label{fig:valence-arousal}
\end{figure}

The area of image emotion and sentiment classification has been explored in a number of ways, primarily through image feature analysis derived from art and psychological factors \citep{machajdik2010affective}; and more recently using techniques such as deep neural networks \citep{chen2015learning, kim2018building}.
Feature extraction and analysis has been used for various applications such as measuring aesthetic appeal \citep{den2010using,den2010comparing,den2011evolving} and as an emotional feature vector for sentiment classification \citep{machajdik2010affective}.
Due to the artistic and psychological underpinnings used by \citet{machajdik2010affective}, the low-level features extracted from images can be understood at a high level.
The relationship between an image's emotion and its core artistic components such as balance, harmony, and variety was further explored by \citet{zhao2014exploring}, which uses a comparably small feature vector to \citet{machajdik2010affective}, resulting however in a 5\% classification increase to state-of-the-art approaches at the time.

Deep neural networks in this domain provide less transparency to the process with which emotions and sentiment are classified compared to feature analysis.
The emotional content of an image can be decomposed in various ways.
Image databases with singular emotion labels, and adjective-noun pairs (ANP) have been used for the training of deep neural network classifiers \citep{chen2014deepsentibank, yang2018visual} with up to 200\% performance gains over support vector machine classifiers.


\section{Methodology}

\begin{todolist}
	\item Emotion profile representation
	\begin{todolist}
		\item Discrete feature vector
		\item Continuous valence-arousal space
	\end{todolist}
	\item System architecture (training/evaluation method)
	\begin{todolist}
		\item Generator: input type (related to emotion profile representation)
		\item Generator: base architecture e.g. use ArtGAN \citep{tan2017artgan}
		\item Discriminator: use output image's emotion classification and error from target emotional profile as error function (autoencoder method)
		\item Discriminator: use image label as target emotional profile and use standard logistic discrimination.
	\end{todolist}
	\item Any interaction made between human and generator e.g. text-to-image synthesis using text emotion classification fed through to the generative model.
	
\end{todolist}

\subsection{Datasets}
There exist a few datasets in which non-facial images have labels of their affective emotion on the viewer.
One particularly of interest to this research is the \textit{WikiArt Emotions} dataset \citep{mohammad2018wikiart} in which images of artwork were labelled with an emotional profile.
Each of the more than 4000 images in the dataset has an associated vector representing the proportion of viewers assigning a given emotion to the artwork.
Emotions such as gratitude, happiness, anger, and arrogance are represented among the twenty emotions assigned to the images.
Along with their respective emotional profile, each image is classified according to its artistic category (Impressionism, Baroque, etc.) and other desirable measures such as viewer rating, artwork title, artist name, and year of creation.
Due to the level of detail relating to each artwork's affective emotional profile, and its classified style and category, this dataset will be investigated initially for creating the generative system detailed below.

\subsection{Emotion profile representation}
The method with which an emotional classification can be represented has been investigated as mentioned in the background section.
With options ranging from a single target label (happy, sad, etc.), to the continuous two-dimensional circumplex model (valence-arousal) representation first introduced by \citet{russell1980circumplex}.
A model for representing emotion commonly used in classification tasks is that of a single label target, due its simplicity.
However due to the subjectivity involved with the emotional classification of an image, a floating-point vector models the relative proportions with which an image's emotion is classified.
Such a model is used explicitly by \citet{ali2017emotional} as a target emotion profile, and is the representation used by \citep{mohammad2018wikiart} due to statistical methods used in gathering data.
The dataset created by \citet{mohammad2018wikiart} uses this representation of emotion to label the artwork available on WikiArt, and given it's relevance to both the domain of art, and emotion, this will be the representation first investigated.

The circumplex model of emotion will be investigated further with both the WikiArt Emotion dataset, which will use the valence-arousal (VA) decomposition presented by \citet{kim2018building} to evaluate the VA equivalence for each artwork.
The process to convert the existing dataset's floating-point vector labels for emotion, to their respective VA values will involve applying the VA decomposition network to each of the dataset's images.
Having both original vector and VA labels allows a direct comparison of the system's performance with both representations.
A dataset has been created by \citet{zhao2016predicting} in which over 1,400,000 images are labelled according to the valence, arousal, and dominance values using image description text analysis.
This dataset can be used in training an predictor architecture according to \citet{kim2018building} that can determine the VA values of a given image.
This can then be applied to both the WikiArt dataset,and extended to a larger number of images available on WikiArt.

\subsection{Generative architecture}
The architecture through which images will be generated represents a core component of the entire creative system.
\citet{tan2017artgan} specifies a GAN architecture which allows the propagation of a target style vector, and a process through which the assigned discriminator style label can be backpropagated to the generator network.
Using such an architecture further allows the incorporation of emotion into the generator input.


\section{Expected Outcomes \& Contributions}

The outcomes of this project will include a generative system with which art can be synthesized according to a target emotional profile.
This system will be the combination of methods for the representation of emotion for use in a generative model, and an architecture with which such a model can be trained.
Due to the exploratory nature of the project with respect to both system architecture and emotional profile representation, this research will have tested and analyzed various options and any comparative differences.

The proposed generative system will be used to create a collection of art, categorised by the target emotional profile with which they were seeded.
The verification proposed involves the public exhibition of produced images, providing feedback to the generative process and pairing the generated images with a human assigned emotion label for use in any further system training.


\bibliographystyle{apalike}
\bibliography{references}

\end{document}
