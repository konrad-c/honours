\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{pifont}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{amssymb}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
	\hspace{-2.5pt}}

\title{Synthesizing emotive art}
\author{Konrad Cybulski}
\date{March 2019}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}
	
\maketitle


\section{Introduction}

For centuries artists have been extremely talented at creating pieces of artwork that convey a range of emotions to those who view them.
Extensive research has been conducted into how visual features affect humans emotionally and how these can be used to predict and detect the emotional content of images and text \citep{machajdik2010affective, zhao2014exploring}.
Due to the subjective and qualitative nature of human emotion, assigning a quantitative measure of emotion to an image is no easy task.
Furthermore the ability to computationally recognise the emotional content of an image has wide-ranging applications from classifying posts on social media, to the creation of images, text, and even physical spaces in an emotionally quantifiable way.

Methods for representing emotion in a quantifiable way has been explored thoroughly in the domain of psychology and medicine, with continuous multi-dimensional representations being used in lieu of a single emotional label \citep{russell1980circumplex}.
The circumplex model of emotion introduced a two-dimensional space characterised by valence, and arousal; respectively representing positivity or negativity, and the level of excitement associated with it.
Such a continuous model is not without flaw, failing to accurately capture more complex emotions that often represent concurrent conflicting sides of a given axis \citep{larsen1992promises}.
The complexity of such a continuous representation of emotion has been extended by \citet{bradley1994measuring} through the addition of a third dimension: dominance; which is particularly of interest within social dynamics.

The domain of emotion classification has had a particular focus on facial expressions and text \citep{cambria2016affective, warriner2013norms}.
Image emotion classifiers have been explored \citep{kim2018building, machajdik2010affective, chen2015learning, chen2014deepsentibank} yet their use has been limited.
While humans ability to recognise, label, and discuss the emotive content of an image is not lacking, the understanding of computational classifiers is.




In the domain of creative artificial intelligence and evolutionary art, the desire exists for processes that produce imagery that is not only visually appealing, but images that exhibit abstract and emotive characteristics.
There exists extensive research on the production of realistic, and target label accurate images.
The use of quality-diverse (QD) algorithms in combination with deep neural network (DNN) image classifiers were used by \citet{nguyen2015innovation} to generate images with high classification accuracy from pre-trained DNN.
\citet{bao2017cvae} exemplifies recent use of variational generative adversarial network (GAN) architectures for the generation of realistic images from fine-grained target labels.
GANs have shown their use in text-to-image synthesis \citep{reed2016generative, zhang2017stackgan}, producing realistic images reflecting the detailed description from which they are generated.
With regard to creative image generation, \citet{tan2017artgan} explored techniques for synthesising artwork with more abstract characteristics.
Through the use of a target artist or genre, the generative network produced images that were highly abstract and stylistically accurate.

Little exploration however has been done on incorporating emotion into the process of art and image generation.
\citet{ali2017emotional} explored the idea of \textit{emotion transfer}, using techniques such as image emotion assignment, and colour/style transfer with the aim of altering image composition to reach a target emotion.
Examples given use a target profile, with varying levels of emotions such as joy, anger, and fear, to alter the image's colour composition.
The classification of an image's affective emotion, the emotion with which a viewer classifies the image, has been explored in various works \citep{machajdik2010affective, chen2015learning, kim2018building}.
\citet{kim2018building} produced a classifier for recognising the emotion attributed to an image.
This was done through the application of a DNN to decompose an image to a two-dimensional feature vector (valence and arousal) representing the image's emotion mapped to a continuous plane (see Figure \ref{fig:valence-arousal}).

Recent work by \citet{nguyen2015innovation} in evolutionary image generation has aimed to address the desire for a system that not only produces visually appealing images, but a diverse collection.
To address the need for a high quality, yet diverse solution space in related optimisation domains, algorithms such as Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) \citep{mouret2015illuminating} and Novelty Search \citep{lehman2008exploiting, lehman2011abandoning} have been developed.
The use of such QD algorithms has shown great promise in its efficiency and accuracy on a number of hard optimisation problems \citep{pugh2016quality} such as maze navigation \citep{lehman2011abandoning}.
\citet{nguyen2015deep} and \citet{nguyen2015innovation} use MAP-Elites in conjunction with a pre-trained deep neural network (DNN) image classifier by assigning individual fitness according to the accuracy with which a generated image is classified.
This process enables exploration into the inner workings of the DNN classifier by uncovering visual patterns that maximise the confidence with which an image is labelled.
\citet{nguyen2015deep} leverages such an architecture to show shallowness with which an image classifier recognises images: recognising alternating yellow and black lines as a \textit{school bus}.


\section{Motivation}



\section{Aims}
The aim of this research is to better understand the visual patterns associated with various emotions conveyed in images.
This will primarily leverage image emotion recognition architectures explored by \citet{kim2018building}, in combination with the dataset produced by \citet{zhao2016predicting} containing over 1.4 million images with assigned valence, arousal, and dominance levels derived from their descriptions.
Producing an architecture with which valence-arousal (VA) values can be assigned to images forms the basis for this research.
Such a platform allows further exploration into the way in which it assigns VA values to more complex, multi-faceted and multi-layered images, and the efficacy with which this is done.
Furthermore this process, and the patterns learned by it can be better understood through the use of generative processes which maximise given target features in a quality-diverse way \citep{nguyen2015innovation, nguyen2015deep}, or through a generative adversarial approach \citep{tan2017artgan}.
In the context of this research, such features include valence, arousal, dominance, happiness, sadness, etc.
This will allow a great understanding of the visual patterns that such an architecture learns, and any psychological or artistic parallels that can be drawn.
The combination of such a generative system with a classifier of emotion can be further extended to domains such as generative art and text-to-image synthesis, with the focus of emotion-driven image generation.

\section{Background}



\subsection{Image emotion recognition}
\begin{todolist}
	\item Sentiment classification: text \& images.
	\item Emotion classification in images: facial expression, general imagery.
	\item Methods of classifying emotion in images: single target emotion, discrete categorical likelihood, decomposition into continuous vector (valence-arousal).
\end{todolist}

\begin{figure}[h!]
	\includegraphics[width=0.75\textwidth]{images/valence-arousal-grid.png}
	\caption{Distribution of emotions associated with levels of valence and arousal determined by DNN classifier produced by \citet{kim2018building}}
	\label{fig:valence-arousal}
\end{figure}

The area of image emotion and sentiment classification has been explored in a number of ways, primarily through image feature analysis derived from art and psychological factors \citep{machajdik2010affective}; and more recently using techniques such as deep neural networks \citep{chen2015learning, kim2018building}.
Feature extraction and analysis has been used for various applications such as measuring aesthetic appeal \citep{den2010using,den2010comparing,den2011evolving} and as an emotional feature vector for sentiment classification \citep{machajdik2010affective}.
Due to the artistic and psychological underpinnings used by \citet{machajdik2010affective}, the low-level features extracted from images can be understood at a high level.
The relationship between an image's emotion and its core artistic components such as balance, harmony, and variety was further explored by \citet{zhao2014exploring}, which uses a comparably small feature vector to \citet{machajdik2010affective}, resulting however in a 5\% classification increase to state-of-the-art approaches at the time.

Deep neural networks in this domain provide less transparency to the process with which emotions and sentiment are classified compared to feature analysis.
The emotional content of an image can be decomposed in various ways.
Image databases with singular emotion labels, and adjective-noun pairs (ANP) have been used for the training of deep neural network classifiers \citep{chen2014deepsentibank, yang2018visual} with up to 200\% performance gains over support vector machine classifiers.


\subsection{Computational image synthesis}
\begin{todolist}
	\item Image generation methods explored: evolutionary algorithms, neural networks, line drawing, etc.
	\item Measuring \textit{goodness} of a generated image: realism, abstractness, aesthetic appeal, etc.
	
	\item What is the generative adversarial network, and differences in common architectures
	\item Why are GANs advantageous over the use of target feature analysis (aesthetics, etc.)
	\item Text-to-image
	\item Style transfer \& image-to-image
\end{todolist}

\begin{figure}[h!]
	\includegraphics[width=\textwidth]{images/sims-interactive-image-generation.png}
	\caption{Images generated through the process of interactive evolution introduced by \citet{sims}}
	\label{fig:sims}
\end{figure}

Generative systems have been explored in various domains through numerous techniques.
Examples range from the generation of images and art using evolutionary methods \citep{sims, nevar}, to the synthesis of text to describe a given image through the use of deep neural networks \citep{mathews2016senticap}.
Some of the first \textit{human-in-the-loop} image synthesis systems such as \textit{NEvAr} \citep{nevar} produced greatly impressive images through evolutionary techniques.
Evolutionary art leveraged methods introduced and exemplified by \citet{sims} such as those shown in Figure \ref{fig:sims}.
\citet{sims} proposed using \textit{Lisp} expressions for genotype definitions, which accepted a coordinate (x, y) which could be evaluated into a grayscale or RGB value producing images.
This genotype expression has been used in numerous further research into the process of both supervised and unsupervised image synthesis through evolutionary techniques \citep{nevar, sims, den2011evolving, distributed-evolutionary-art, aesthetic-measures}.

Despite the slow nature of the interactive process, \citet{sims} and \citet{nevar} were able to produce images with visually striking characteristics.
\citet{aesthetic-measures} investigated measures of aesthetics for fitness evaluation in artificially evolving images.
This research primarily used observations by \citet{ralph-bell-curve}, that the distribution of colour gradients in fine art tend towards normal.
While the images produced through this method did not meet the level of intricacy and detail as the results of \citet{sims} or \citet{nevar}, it represented a self-contained system able to generate appealing imagery without human interaction.

Introduction of the generative adversarial network architecture (GAN) by \citet{GAN} allowed the process of image generation to be completely unsupervised.
Common GAN application has involved the generation of realistic images, such as has been done by \citet{bao2017cvae}, where images have been synthesised to fine-detailed target labels such as bird species' and actors.
\citet{zhang2017stackgan} and \citet{reed2016generative} have recently explored text to image synthesis, in which detailed descriptions of birds and flowers have been converted into photo-realistic images using the GAN model.
\citet{tan2017artgan} has explored the generation of art according to target genre and artist.


As mentioned by \citet{nguyen2015innovation} in the context of an \textit{Innovation Engine}, high fitness individuals for a given target classification label (e.g. water tower, volcano, dome, etc.) often arise from varying fitness individuals in a different domain.
The example shown by \citet{nguyen2015innovation} shows the path taken by numerous images descending from the classification of \textit{abaya} with 47\% confidence.
A \textit{volcano} (99\%)  image descended from a \textit{castle} image (4\%), a \textit{planetarium} (95\%) from a \textit{boathouse} (10\%), a \textit{beacon} (96\%) from a \textit{cocker spaniel} (2\%).
The evolutionary trajectory of individuals does not resemble the path taken in common genetic algorithms, where high fitness individuals arise from the exploitation of a local optima.
The evolutionary path in this context conflicts with the findings of \citet{mouret2015illuminating} in which the MAP-Elites algorithm applied to neural network optimisation resulted in the fittest individuals arising due to mutations of their direct parents in the feature-space.

The application of generative systems in the domain of affective computing, particularly with regard to emotion and content synthesis is limited.
\textit{Emotion transfer} was explored by \citet{ali2017emotional} which involved the transformation of an image's colour and style with the aim of altering its conveyed emotion.


\section{Methodology}

\begin{todolist}
	\item Emotion profile representation
	\begin{todolist}
		\item Discrete feature vector
		\item Continuous valence-arousal space
	\end{todolist}
	\item System architecture (training/evaluation method)
	\begin{todolist}
		\item Generator: input type (related to emotion profile representation)
		\item Generator: base architecture e.g. use ArtGAN \citep{tan2017artgan}
		\item Discriminator: use output image's emotion classification and error from target emotional profile as error function (autoencoder method)
		\item Discriminator: use image label as target emotional profile and use standard logistic discrimination.
	\end{todolist}
	\item Any interaction made between human and generator e.g. text-to-image synthesis using text emotion classification fed through to the generative model.
	
\end{todolist}

\subsection{Datasets}
There exist a few datasets in which non-facial images have labels of their affective emotion on the viewer.
One particularly of interest to this research is the \textit{WikiArt Emotions} dataset \citep{mohammad2018wikiart} in which images of artwork were labelled with an emotional profile.
Each of the more than 4000 images in the dataset has an associated vector representing the proportion of viewers assigning a given emotion to the artwork.
Emotions such as gratitude, happiness, anger, and arrogance are represented among the twenty emotions assigned to the images.
Along with their respective emotional profile, each image is classified according to its artistic category (Impressionism, Baroque, etc.) and other desirable measures such as viewer rating, artwork title, artist name, and year of creation.
Due to the level of detail relating to each artwork's affective emotional profile, and its classified style and category, this dataset will be investigated initially for creating the generative system detailed below.

\subsection{Emotion profile representation}
The method with which an emotional classification can be represented has been investigated as mentioned in the background section.
With options ranging from a single target label (happy, sad, etc.), to the continuous two-dimensional circumplex model (valence-arousal) representation first introduced by \citet{russell1980circumplex}.
A model for representing emotion commonly used in classification tasks is that of a single label target, due its simplicity.
However due to the subjectivity involved with the emotional classification of an image, a floating-point vector models the relative proportions with which an image's emotion is classified.
Such a model is used explicitly by \citet{ali2017emotional} as a target emotion profile, and is the representation used by \citep{mohammad2018wikiart} due to statistical methods used in gathering data.
The dataset created by \citet{mohammad2018wikiart} uses this representation of emotion to label the artwork available on WikiArt, and given it's relevance to both the domain of art, and emotion, this will be the representation first investigated.

The circumplex model of emotion will be investigated further with both the WikiArt Emotion dataset, which will use the valence-arousal (VA) decomposition presented by \citet{kim2018building} to evaluate the VA equivalence for each artwork.
The process to convert the existing dataset's floating-point vector labels for emotion, to their respective VA values will involve applying the VA decomposition network to each of the dataset's images.
Having both original vector and VA labels allows a direct comparison of the system's performance with both representations.
A dataset has been created by \citet{zhao2016predicting} in which over 1,400,000 images are labelled according to the valence, arousal, and dominance values using image description text analysis.
This dataset can be used in training an predictor architecture according to \citet{kim2018building} that can determine the VA values of a given image.
This can then be applied to both the WikiArt dataset,and extended to a larger number of images available on WikiArt.

\subsection{Generative architecture}
The architecture through which images will be generated represents a core component of the entire creative system.
\citet{tan2017artgan} specifies a GAN architecture which allows the propagation of a target style vector, and a process through which the assigned discriminator style label can be backpropagated to the generator network.
Using such an architecture further allows the incorporation of emotion into the generator input.


\section{Expected Outcomes \& Contributions}

The outcomes of this project will include a generative system with which art can be synthesized according to a target emotional profile.
This system will be the combination of methods for the representation of emotion for use in a generative model, and an architecture with which such a model can be trained.
Due to the exploratory nature of the project with respect to both system architecture and emotional profile representation, this research will have tested and analyzed various options and any comparative differences.

The proposed generative system will be used to create a collection of art, categorised by the target emotional profile with which they were seeded.
The verification proposed involves the public exhibition of produced images, providing feedback to the generative process and pairing the generated images with a human assigned emotion label for use in any further system training.


\bibliographystyle{apalike}
\bibliography{references}

\end{document}
