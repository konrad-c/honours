\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{pifont}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{amssymb}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
	\hspace{-2.5pt}}

\title{Exploring emotional representation in deep neural network image emotion classifiers}
\author{Konrad Cybulski}
\date{March 2019}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}
	
\maketitle


\section{Introduction}

For centuries artists have been extremely talented at creating pieces of artwork that convey a range of emotions to those who view them.
Extensive research has been conducted into how visual features affect humans emotionally and how these can be used to predict and detect the emotional content of images and text \citep{machajdik2010affective, zhao2014exploring}.
Due to the subjective and qualitative nature of human emotion, assigning a quantitative measure of emotion to an image is no easy task.
Furthermore the ability to computationally recognise the emotional content of an image has wide-ranging applications from classifying posts on social media, to the creation of images, text, and even physical spaces in an emotionally quantifiable way.

Methods for quantifiably representing emotion have been explored thoroughly in the domain of psychology, with continuous multi-dimensional models being used in lieu of a single emotional label .
The circumplex model of emotion introduced a two-dimensional space characterised by valence, and arousal; respectively representing positivity or negativity, and the level of excitement associated with it \citep{russell1980circumplex}.
Such a continuous model is not without flaw, failing to accurately capture more complex emotional states, often those that represent concurrent conflicting sides of a given axis \citep{larsen1992promises}.
The complexity of such a continuous representation of emotion has been investigated in depth and extended in various ways such as by \citet{bradley1994measuring}, through the addition of a third dimension: dominance; which is particularly of interest within social dynamics.

The domain of emotion classification has had a particular focus on facial expressions and text \citep{cambria2016affective, warriner2013norms}.
Image emotion classifiers have been explored \citep{kim2018building, machajdik2010affective, chen2015learning, chen2014deepsentibank} yet their use has been limited.
While humans ability to recognise, label, and discuss the emotive content of an image is not lacking, the capability to computationally classify image emotion, and the underlying patterns learnt by such classifiers is.

As with the use of deep neural network image classifiers such as ResNet, AlexNet, and Inception, the shapes and patterns learnt by such deep learning systems are difficult to extract.
Recent work has looked at the use of quality-diverse generative algorithms with such deep classifiers \citep{nguyen2015deep, nguyen2015innovation}.
The underlying patterns learned by the classifier can be surfaced by synthesising images that maximise various desirable features.
This method allowed the exploration of images to which the classifier assigns a given label such as \textit{school bus} or \textit{lighthouse}, enabling a deeper understanding of the visual characteristics inherent to each class.

\section{Aims}
The aim of this research is to better understand the visual patterns associated with various emotions conveyed in images.
This will primarily leverage image emotion recognition architectures explored by \citet{kim2018building}, in combination with the dataset produced by \citet{zhao2016predicting} containing over 1.4 million images with assigned valence, arousal, and dominance levels derived from their descriptions.
Producing an architecture with which valence-arousal (VA) values can be assigned to images forms the basis for this research.
Such a platform allows enables the exploration into how VA values are assigned to more complex, multi-faceted and multi-layered images, and the efficacy with which this is done.
Furthermore this process, and the patterns learned by it will be better understood through the use of generative processes which maximise given target features in a quality-diverse way \citep{nguyen2015innovation, nguyen2015deep}, or through a generative adversarial approach \citep{tan2017artgan}.
In the context of this research, such features include valence, arousal, dominance, happiness, sadness, etc.
This will allow a great understanding of the visual patterns that such an architecture learns, and any psychological or artistic parallels that can be drawn.
The combination of such a generative system with a classifier of emotion can be further extended to domains such as generative art and text-to-image synthesis, with a focus on emotion-driven image generation.


\section{Background}

\subsection{Image emotion recognition}
\begin{todolist}
	\item Sentiment classification: text \& images.
	\item Emotion classification in images: facial expression, general imagery.
	\item Methods of classifying emotion in images: single target emotion, discrete categorical likelihood, decomposition into continuous vector (valence-arousal).
\end{todolist}

\begin{figure}[h!]
	\includegraphics[width=0.75\textwidth]{images/valence-arousal-grid.png}
	\caption{Distribution of emotions associated with levels of valence and arousal determined by DNN classifier produced by \citet{kim2018building}}
	\label{fig:valence-arousal}
\end{figure}

The area of image emotion and sentiment classification has been explored in a number of ways, primarily through image feature analysis derived from art and psychological factors \citep{machajdik2010affective}; and more recently using techniques such as deep neural networks \citep{chen2015learning, kim2018building}.
Feature extraction and analysis has been used for various applications such as measuring aesthetic appeal \citep{den2010using,den2010comparing,den2011evolving} and as an emotional feature vector for sentiment classification \citep{machajdik2010affective}.
Due to the artistic and psychological underpinnings used by \citet{machajdik2010affective}, the low-level features extracted from images can be understood at a high level.
The relationship between an image's emotion and its core artistic components such as balance, harmony, and variety was further explored by \citet{zhao2014exploring}, which uses a comparably small feature vector to \citet{machajdik2010affective}, resulting however in a 5\% classification increase to state-of-the-art approaches at the time.

Deep neural networks in this domain provide less transparency to the process with which emotions and sentiment are classified compared to feature analysis.
The emotional content of an image can be decomposed in various ways.
Image databases with singular emotion labels, and adjective-noun pairs (ANP) have been used for the training of deep neural network classifiers \citep{chen2014deepsentibank, yang2018visual} with up to 200\% performance gains over support vector machine classifiers.


\subsection{Computational image synthesis}
\begin{todolist}
	\item[\done] Image generation methods explored: evolutionary algorithms, neural networks, line drawing, etc.
	\item[\done] Measuring \textit{goodness} of a generated image: realism, abstractness, aesthetic appeal, etc.
	
	\item What is the generative adversarial network, and differences in common architectures
	\item Why are GANs advantageous over the use of target feature analysis (aesthetics, etc.)
	\item Text-to-image
	\item Style transfer \& image-to-image
\end{todolist}

\begin{figure}[h!]
	\includegraphics[width=\textwidth]{images/sims-interactive-image-generation.png}
	\caption{Images generated through the process of interactive evolution introduced by \citet{sims}}
	\label{fig:sims}
\end{figure}

Generative systems have been explored in various domains with numerous techniques.
Examples range from the generation of images and art using evolutionary methods \citep{sims, nevar}, to the synthesis of text to describe a given image through the use of deep neural networks \citep{mathews2016senticap}.
Some of the first \textit{human-in-the-loop} image synthesis systems such as \textit{NEvAr} \citep{nevar} produced greatly impressive images through evolutionary techniques.
Evolutionary art leveraged methods introduced and exemplified by \citet{sims} such as those shown in Figure \ref{fig:sims}.
\citet{sims} proposed using \textit{Lisp} expressions for genotype definitions, which accepted a coordinate (x, y) which could be evaluated into a grayscale or RGB value producing images.
This genotype expression has been used in numerous further research into the process of both supervised and unsupervised image synthesis through evolutionary techniques \citep{nevar, sims, den2011evolving, distributed-evolutionary-art, aesthetic-measures}.

Despite the slow nature of the interactive process, \citet{sims} and \citet{nevar} were able to produce images with visually striking characteristics.
\citet{aesthetic-measures} investigated measures of aesthetics for fitness evaluation in artificially evolving images.
This research primarily used observations by \citet{ralph-bell-curve}, that the distribution of colour gradients in fine art tend towards normal.
While the images produced through this method did not meet the level of intricacy and detail as the results of \citet{sims} or \citet{nevar}, it represented a self-contained system able to generate appealing imagery without human interaction.

Introduction of the generative adversarial network architecture (GAN) by \citet{GAN} allowed the process of image generation to depend only on collecting a sufficiently large dataset.
Common GAN application has involved the generation of realistic images, such as has been done by \citet{bao2017cvae}, where images have been synthesised to fine-detailed target labels such as bird species' and actors.
\citet{zhang2017stackgan} and \citet{reed2016generative} have recently explored text to image synthesis, in which detailed descriptions of birds and flowers have been converted into photo-realistic images using the GAN model.
Such an architecture has been applied to the area of art synthesis by \citet{tan2017artgan} in which images were generated according to a target genre and artist.
Learning from a dataset of countless artworks under various categories, styles, and artists, it was hugely successful in generating images that were stylistically similar to existing art of the target artist/genre.
Due to the competitive relationship of the generator and discriminator networks, the patterns learned by the discriminator propagate through the generator network.
The discriminator network of the GAN architecture aims to learn patterns and styles from the dataset on which it is trained in order to discriminate between the generated and existing images.
As a result, the images generated tend to resemble closely those in the training dataset, which represents a benefit in successfully producing images closely matching the target domain, and a detriment with regard to the networks potential creativity.

Recent work by \citet{nguyen2015innovation} and \citet{nguyen2015deep} investigated the use of quality-diverse algorithms for image generation particularly to better understand the patterns learned by deep neural network image classifiers.
Quality-diverse (QD) evolutionary algorithms such as Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) \citep{mouret2015illuminating} and Novelty Search \citep{lehman2008exploiting, lehman2011abandoning} have been developed to address the need for a high quality, yet diverse solution space in related optimisation domains.
The use of such QD algorithms has shown great promise in its efficiency and accuracy on a number of hard optimisation problems \citep{pugh2016quality} such as maze navigation \citep{lehman2011abandoning}.
\citet{nguyen2015deep} and \citet{nguyen2015innovation} use MAP-Elites in conjunction with a pre-trained deep neural network (DNN) image classifier; assigning individual image fitness according to the accuracy with which it is classified.
Using the MAP-Elites framework in this context, each dimension of the feature-space represents a classification label, and as such the generated images allow the exploration of label representative patterns and shapes learnt by the classifier.
\citet{nguyen2015deep} leverages such an architecture to show the shallowness with which an image classifier recognises images.
Assigning the label of \textit{school bus} to alternating yellow and black lines is a prime example of the way such a network has learnt to differentiate one class from the others.
Thus enabling exploration into the inner workings of the DNN classifier by uncovering features that maximise the separation of one label to another.
In contrast, \citet{nguyen2015innovation} uses the same architecture to explore the novelty-driven evolutionary path taken by generated images and the potential for such a system in the field of content synthesis.
While the conclusions derived from \citet{nguyen2015innovation} and \citet{nguyen2015deep} contrast greatly, the quality-diverse generative method used to understand the visual components learnt by the classifier show such an architecture's exploratory abilities.


The application of generative systems in the domain of affective computing, particularly with regard to emotion and content synthesis, is limited.
\textit{Emotion transfer} was explored by \citet{ali2017emotional} which involved the transformation of an image's colour and style with the aim of altering its conveyed emotion.


\section{Methodology}

\begin{todolist}
	\item[\done] Datasets
	\item Emotion profile representation
	\begin{todolist}
		\item Discrete feature vector
		\item Continuous valence-arousal space
	\end{todolist}
	\item System architecture (training/evaluation method)
	\begin{todolist}
		\item Generator: input type (related to emotion profile representation)
		\item Generator: base architecture e.g. use ArtGAN \citep{tan2017artgan}
		\item Discriminator: use output image's emotion classification and error from target emotional profile as error function (autoencoder method)
		\item Discriminator: use image label as target emotional profile and use standard logistic discrimination.
	\end{todolist}
	\item Any interaction made between human and generator e.g. text-to-image synthesis using text emotion classification fed through to the generative model.
	
\end{todolist}

\subsection{Datasets}

There exist a few datasets in which non-facial images have labels of their affective emotion on the viewer.
Produced recently by \citep{zhao2016predicting} is a compilation of 1.4 million images from \textit{Flickr}, each assigned with a respective valence, arousal, and dominance (VAD) according to the circumplex model of emotion \citep{russell1980circumplex, bradley1994measuring}.
The assigned values are derived from the analysis of each images textual description according to the methods described by \citet{warriner2013norms}.
Additionally within the dataset are respective VAD values for the comments written by other users.
This dataset has been chosen due to its large volume of data, and the method in which each VAD value is assigned.
As detailed in \citet{zhao2016predicting}, the method for assigning values to each image involved the computational analysis of an image's description, in addition to a human filtering step to ensure the relative accuracy and validity of the assigned values.

As previously discussed, there exist issues with the circumplex model of emotion.
There exist shortcomings particularly in both its ability to capture more complex emotion, and the ease with which humans are able to convert felt emotions to a continuous space.
The value in using such a large dataset is to overcome any potential error or inaccuracy associated with the assigned VAD values.

\subsection{Emotion profile representation}
The method with which an emotional classification can be represented has been investigated as mentioned in the background section.
With options ranging from a single target label (happy, sad, etc.), to the continuous two-dimensional circumplex model (valence-arousal) representation first introduced by \citet{russell1980circumplex}.
A model for representing emotion commonly used in classification tasks is that of a single label target, due its simplicity.
However due to the subjectivity involved with the emotional classification of an image, a floating-point vector models the relative proportions with which an image's emotion is classified.
Such a model is used explicitly by \citet{ali2017emotional} as a target emotion profile, and is the representation used by \citep{mohammad2018wikiart} due to statistical methods used in gathering data.
The dataset created by \citet{mohammad2018wikiart} uses this representation of emotion to label the artwork available on WikiArt, and given it's relevance to both the domain of art, and emotion, this will be the representation first investigated.

The circumplex model of emotion will be investigated further with both the WikiArt Emotion dataset, which will use the valence-arousal (VA) decomposition presented by \citet{kim2018building} to evaluate the VA equivalence for each artwork.
The process to convert the existing dataset's floating-point vector labels for emotion, to their respective VA values will involve applying the VA decomposition network to each of the dataset's images.
Having both original vector and VA labels allows a direct comparison of the system's performance with both representations.
A dataset has been created by \citet{zhao2016predicting} in which over 1,400,000 images are labelled according to the valence, arousal, and dominance values using image description text analysis.
This dataset can be used in training an predictor architecture according to \citet{kim2018building} that can determine the VA values of a given image.
This can then be applied to both the WikiArt dataset,and extended to a larger number of images available on WikiArt.

\subsection{Generative architecture}
The architecture through which images will be generated represents a core component of the entire creative system.
\citet{tan2017artgan} specifies a GAN architecture which allows the propagation of a target style vector, and a process through which the assigned discriminator style label can be backpropagated to the generator network.
Using such an architecture further allows the incorporation of emotion into the generator input.


\section{Expected Outcomes \& Contributions}

The outcomes of this project will include a generative system with which art can be synthesized according to a target emotional profile.
This system will be the combination of methods for the representation of emotion for use in a generative model, and an architecture with which such a model can be trained.
Due to the exploratory nature of the project with respect to both system architecture and emotional profile representation, this research will have tested and analyzed various options and any comparative differences.

The proposed generative system will be used to create a collection of art, categorised by the target emotional profile with which they were seeded.
The verification proposed involves the public exhibition of produced images, providing feedback to the generative process and pairing the generated images with a human assigned emotion label for use in any further system training.


\bibliographystyle{apalike}
\bibliography{references}

\end{document}
